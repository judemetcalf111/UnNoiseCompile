{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0445598",
   "metadata": {},
   "source": [
    "# Tutorial on how to use the \"Efficient DeNoising\" of this Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = {\n",
    "    \"000\": 100,\n",
    "    \"001\": 50,\n",
    "    \"010\": 75,\n",
    "    \"011\": 25,\n",
    "    \"111\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import measfilter as mf\n",
    "def eff_DeNoise(datadict, meas_filter, percentage=99, verbose=True, GD = False, lr=0.1, max_iter=50):\n",
    "    \"\"\"\n",
    "    Efficient DeNoiser function that applies the SplitMeasFilter to the provided data dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - datadict: Dictionary containing measurement data.\n",
    "    - SplitMeasFilter: An instance of the SplitMeasFilter class with calibrated errors.\n",
    "    - percentage: percentage threshold for filtering (default is 99).\n",
    "    - verbose: If True, prints progress information.\n",
    "    - GD: If True, performs optional Gradient Descent refinement.\n",
    "    - lr: Learning rate for Gradient Descent (if GD is True).\n",
    "    - max_iter: Maximum number of iterations for Gradient Descent (if GD is True).\n",
    "    \n",
    "    Returns:\n",
    "    - denoised_data: Dictionary containing denoised measurement data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We only sum over source bitstrings that are most computationally significant\n",
    "    # Defined from the `percentage` parameter.\n",
    "    important_dict = mf.dict_filter(datadict, percent=percentage)\n",
    "    \n",
    "    input_len = len(important_dict)\n",
    "    # Defining our targets in a sparse approach, only those significant sources \n",
    "    # of the bitstrings we actually saw will remain (noise won't make our output vanish completely!)\n",
    "    targets = list(important_dict.keys())\n",
    "    \n",
    "    denoised_data = {k: 0.0 for k in targets}\n",
    "    \n",
    "    # Sparse matrix mult (a (pretty unavoidable) double loop over sources and targets)\n",
    "    # Equation: P_denoised(target) = Sum_over_sources( M_inv[target, source] * P_noisy(source) )\n",
    "    \n",
    "    # Pre-calculate total shots for normalization later\n",
    "    filtered_shots = sum(important_dict.values())\n",
    "    \n",
    "    for t_str in targets:\n",
    "        new_prob = 0.0\n",
    "        for s_str, s_count in important_dict.items():\n",
    "            # Calculate probability of Source(s) flipping to Target(t)\n",
    "            # This is the \"Likelihood of changing/remaining\"\n",
    "            weight = meas_filter.get_inverse_element(t_str, s_str) # M_inv(target_bitstring=t_str, source_bitstring=s_str)\n",
    "            \n",
    "            # Add contribution: (Inverse Element) * (Observed Probability)\n",
    "            s_prob = s_count / filtered_shots\n",
    "            new_prob += weight * s_prob\n",
    "            \n",
    "        denoised_data[t_str] = new_prob\n",
    "\n",
    "    # Constrained optimisation using clipping, a good heuristic.\n",
    "    # We minimize ||Ap - p_tilde|| clipping to to p >= 0 and re-normalizing.\n",
    "    \n",
    "    final_data = {}\n",
    "    sum_p = 0.0\n",
    "    \n",
    "    for k, v in denoised_data.items():\n",
    "        if v > 1e-9: # Clip negatives and near-zeros\n",
    "            final_data[k] = v\n",
    "            sum_p += v\n",
    "            \n",
    "    # Re-normalize to ensure sum is 1.0 (or original shot count)\n",
    "    if sum_p > 0:\n",
    "        factor = filtered_shots / sum_p\n",
    "        final_data = {k: v * factor for k, v in final_data.items()}\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OPTIONAL: GRADIENT DESCENT REFINEMENT\n",
    "    # -------------------------------------------------------------------------\n",
    "    if GD:\n",
    "        if verbose: print(f\"Analytic pass done. Starting Gradient Descent refinement...\")\n",
    "        \n",
    "        n_dim = len(targets)\n",
    "        \n",
    "        # A. Vectorize Data\n",
    "        # 'y' = The actual noisy observations we want to match\n",
    "        p_noisy_obs = np.array([important_dict[t] for t in targets]) / filtered_shots\n",
    "        \n",
    "        # 'x' = Our initial guess (The result from the Analytic pass)\n",
    "        # Using the analytic result as a \"warm start\" makes GD extremely fast.\n",
    "        p_est = np.zeros(n_dim)\n",
    "        for i, t in enumerate(targets):\n",
    "            p_est[i] = final_data.get(t, 0.0) / filtered_shots\n",
    "        \n",
    "        # B. Build Forward Matrix M_sub for this subspace\n",
    "        # We need M (Forward), not M_inv, to calculate the Loss: || M*x - y ||^2\n",
    "        M_sub = np.zeros((n_dim, n_dim))\n",
    "        \n",
    "        for r, obs_bit in enumerate(targets):     # Row: Observed\n",
    "            for c, hid_bit in enumerate(targets): # Col: Hidden (Clean)\n",
    "                M_sub[r, c] = meas_filter.get_forward_element(obs_bit, hid_bit)\n",
    "        \n",
    "        # C. Projected Gradient Descent Loop\n",
    "        for step in range(max_iter-1):\n",
    "            # Forward: p_pred = M * p_est\n",
    "            p_pred = M_sub @ p_est\n",
    "            \n",
    "            # Gradient of MSE: grad = M.T * (p_pred - p_obs)\n",
    "            diff = p_pred - p_noisy_obs\n",
    "            grad = M_sub.T @ diff\n",
    "            \n",
    "            # Update\n",
    "            p_est = p_est - lr * grad\n",
    "            \n",
    "            # Projection (Constraint: p >= 0 and sum(p) = 1)\n",
    "            p_est[p_est < 0] = 0 # Clip negatives\n",
    "            \n",
    "            curr_sum = np.sum(p_est)\n",
    "            if curr_sum > 0:\n",
    "                p_est /= curr_sum # Normalize\n",
    "            \n",
    "            # Convergence Check (Small gradient magnitude)\n",
    "            if np.linalg.norm(lr * grad) < 1e-18:\n",
    "                print(f\"----  GD Converged after {step+1} steps!  ----\")\n",
    "                break\n",
    "\n",
    "            print(f\"----  GD Step {step+1}/{max_iter} complete...  ----\")\n",
    "        \n",
    "        # D. Update final_data with refined values\n",
    "        final_data = {}\n",
    "        for i, t in enumerate(targets):\n",
    "            if p_est[i] > 1e-9:\n",
    "                final_data[t] = p_est[i] * filtered_shots # Scale back to counts\n",
    "    \n",
    "        if verbose:\n",
    "            print(f\"Sparse DeNoising Complete. Gradient Descent Used\")\n",
    "            print(f\"Input Keys: {len(datadict)} -> Filtered Sources: {input_len} -> Output Targets: {len(final_data)}\")\n",
    "            print(f\"Total Shots (Input): {sum(datadict.values())} -> (Filtered): {filtered_shots} -> (Output): {sum(final_data.values())}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Sparse DeNoising Complete. No Gradient Descent included\")\n",
    "            print(f\"Input Keys: {len(datadict)} -> Filtered Sources: {input_len} -> Output Targets: {len(final_data)}\")\n",
    "            print(f\"Total Shots (Input): {sum(datadict.values())} -> (Filtered): {filtered_shots} -> (Output): {sum(final_data.values())}\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    final_data = {k: float(v) for k, v in final_data.items()} # np.float64 -> float\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "738e6e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse DeNoising Complete. No Gradient Descent included\n",
      "Input Keys: 5 -> Filtered Sources: 4 -> Output Targets: 4\n",
      "Total Shots (Input): 251 -> (Filtered): 250 -> (Output): 250.0\n",
      "Analytic pass done. Starting Gradient Descent refinement...\n",
      "----  GD Step 1/50 complete...  ----\n",
      "----  GD Converged after 2 steps!  ----\n",
      "Sparse DeNoising Complete. Gradient Descent Used\n",
      "Input Keys: 5 -> Filtered Sources: 4 -> Output Targets: 4\n",
      "Total Shots (Input): 251 -> (Filtered): 250 -> (Output): 250.0\n"
     ]
    }
   ],
   "source": [
    "from measfilter import SplitMeasFilter\n",
    "my_filter = SplitMeasFilter(qubit_order=[0, 1], file_address='error_data/')\n",
    "my_filter.post_from_file()\n",
    "# print(\"Inverse Matrix Loaded:\", my_filter.inv_matrices_mean[0])\n",
    "clean_counts = eff_DeNoise(data_example, my_filter)\n",
    "clean_counts_gd = eff_DeNoise(data_example, my_filter, GD = True, lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb00208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000': 100, '001': 50, '010': 75, '011': 25, '111': 1}\n",
      "{'000': 99.59990731047057, '010': 75.31628980422414, '001': 50.05765358213608, '011': 25.026149303169205}\n",
      "{'000': 99.59990731047058, '010': 75.31628980422413, '001': 50.05765358213608, '011': 25.02614930316921}\n"
     ]
    }
   ],
   "source": [
    "print(data_example)\n",
    "print(clean_counts)\n",
    "print(clean_counts_gd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnNoiseCompile Kernel",
   "language": "python",
   "name": "unnoisecompile_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
